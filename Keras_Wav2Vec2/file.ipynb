{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 21:32:23.073470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import logging # allows writing status messages to a file or output streams\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Only log error messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "# set random seed\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# tf.keras.utils.set_random_seed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable definition\n",
    "MAX_DURATION = 1 # duration of the input audio file we feed into wav2vec\n",
    "SAMPLING_RATE = 16000 # no. of samples of audio recorded every second\n",
    "BATCH_SIZE = 32 # batch size for training and evaluating the model\n",
    "NUM_CLASSES = 10 # classes our dataset will have\n",
    "HIDDEN_DIM = 768 # Dimension of our model output (768 is for wav2vec2-base)\n",
    "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE # max length of the input audio file\n",
    "# 1 x 16k\n",
    "MAX_FRAMES = 49\n",
    "MAX_EPOCHS = 2 # maximum number of training epochs\n",
    "\n",
    "\n",
    "MODEL_CHECKPOINT = \"facebook/wav2vec2-base-960h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ix502iv/miniconda3/envs/TensorFlow/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset superb (/home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 82.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "speech_command_v1 = load_dataset(\"superb\", \"ks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'label'],\n",
       "        num_rows: 51094\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['file', 'audio', 'label'],\n",
       "        num_rows: 6798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'label'],\n",
       "        num_rows: 3081\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_command_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e/cache-404a78a13bbd3075.arrow and /home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e/cache-7561275829af76ab.arrow\n",
      "Loading cached processed dataset at /home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e/cache-c3de9edf0fb9d9a4.arrow\n",
      "Loading cached processed dataset at /home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e/cache-b53d92383438b4ad.arrow\n"
     ]
    }
   ],
   "source": [
    "# split test dataset by label to handle both train/test: 65/35 train vs test\n",
    "speech_command_v1 = speech_command_v1['train'].train_test_split(train_size=0.65, test_size=0.35, stratify_by_column='label')\n",
    "\n",
    "# remove the unknown and silence classes on the train dataset -> filter method\n",
    "speech_command_v1 = speech_command_v1.filter(\n",
    "    lambda x: x[\"label\"]\n",
    "    != (\n",
    "        speech_command_v1[\"train\"].features[\"label\"].names.index(\"_silence_\")\n",
    "        and speech_command_v1['train'].features[\"label\"].names.index(\"_unknown_\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# sample our train and test dataset splits to a multiple of the bathc size = 2 -> select method\n",
    "speech_command_v1[\"train\"] = speech_command_v1['train'].select(\n",
    "    [i for i in range((len(speech_command_v1[\"train\"]) // BATCH_SIZE) * BATCH_SIZE)]\n",
    ")\n",
    "\n",
    "speech_command_v1[\"test\"] = speech_command_v1['test'].select(\n",
    "    [i for i in range((len(speech_command_v1[\"test\"]) // BATCH_SIZE) * BATCH_SIZE)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'label'],\n",
       "        num_rows: 12032\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'label'],\n",
       "        num_rows: 6464\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_command_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'yes', '1': 'no', '2': 'up', '3': 'down', '4': 'left', '5': 'right', '6': 'on', '7': 'off', '8': 'stop', '9': 'go', '10': '_silence_', '11': '_unknown_'}\n"
     ]
    }
   ],
   "source": [
    "labels = speech_command_v1[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e/cache-e43f185efe57adc3.arrow\n",
      "Loading cached processed dataset at /home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e/cache-fd441042e446b8ba.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e/cache-630164a2d3c007b6.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ix502iv/.cache/huggingface/datasets/superb/ks/1.9.0/b8183f71eabe8c559d7f3f528ab37a6a21ad1ee088fd3423574cecad8b3ec67e/cache-8b113a6b78c54f2f.arrow\n"
     ]
    }
   ],
   "source": [
    "# Import the neccesary libs\n",
    "\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    MODEL_CHECKPOINT, return_attention_mask=True\n",
    ")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate = feature_extractor.sampling_rate,\n",
    "        max_length = MAX_SEQ_LENGTH,\n",
    "        truncation = True,\n",
    "        padding=True,\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "processed_speech_commands_v1 = speech_command_v1.map(\n",
    "    preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True\n",
    ") # drop the audio and file column, they are not neccessary while training\n",
    "\n",
    "# Load the entire dataset and split as a dict of np arrays\n",
    "train = processed_speech_commands_v1[\"train\"].shuffle(seed=42).with_format(\"numpy\")[:]\n",
    "test = processed_speech_commands_v1[\"test\"].shuffle(seed=42).with_format(\"numpy\")[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wav2Vec2.0 / Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFWav2Vec2Model\n",
    "# Model Definition\n",
    "\n",
    "def mean_pool(hidden_states, feature_lengths):\n",
    "    attention_mask = tf.sequence_mask(\n",
    "        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64\n",
    "    )\n",
    "\n",
    "    padding_mask = tf.cast(\n",
    "        tf.reverse(tf.cumsum(tf.reverse(attention_mask, [-1]), -1), [-1]),\n",
    "        dtype=tf.dtypes.bool,\n",
    "    )\n",
    "\n",
    "    hidden_states = tf.where(\n",
    "        tf.broadcast_to(\n",
    "            tf.expand_dims(~padding_mask, -1), (BATCH_SIZE, MAX_FRAMES, HIDDEN_DIM)\n",
    "        ),\n",
    "        0.0,\n",
    "        hidden_states,\n",
    "    ) # returns the indexes of non-zero elements\n",
    "\n",
    "    pooled_state = tf.math.reduce_sum(hidden_states, axis=1)/ tf.reshape(\n",
    "        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),\n",
    "        [-1, 1],\n",
    "    )\n",
    "    return pooled_state\n",
    "    # reduce_sum -> computes the sum of all elements across dimensions of a tensor.\n",
    "\n",
    "class TFWav2Vec2ForAudioClassification(layers.Layer):\n",
    "    \"\"\"\n",
    "    Combines the Encoder and Decoder into an end-to-end model for training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_checkpoint, num_classes):\n",
    "        super(TFWav2Vec2ForAudioClassification, self).__init__()\n",
    "        # Instantiate the Wav2vec2 model w/out the Classification-head\n",
    "        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(\n",
    "            model_checkpoint, apply_spec_augment=False, from_pt=True\n",
    "        )\n",
    "        self.pooling = layers.GlobalAveragePooling1D()\n",
    "\n",
    "        # Drop-out layer before the classification head\n",
    "        self.intermediate_layer_dropout = layers.Dropout(0.5)\n",
    "\n",
    "        # Classification Head\n",
    "        self.final_layer = layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # take the first output in the returned dictionary corresponding to \n",
    "        # the output of the last layer of the Wav2Vec2\n",
    "        hidden_states = self.wav2vec2(inputs[\"input_values\"])[0]\n",
    "\n",
    "        # if attention_mask doesn't exist then mean-pool only unmasked output frames\n",
    "        if tf.is_tensor(inputs[\"attention_mask\"]):\n",
    "            # get the length of each audio input by summing up the attention mask\n",
    "            audio_lengths = tf.cumsum(inputs[\"attention_mask\"], -1)[:,-1]\n",
    "\n",
    "            # get the no. of wav2vec2 output frames for each corresponding audio input \n",
    "            # length\n",
    "            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(\n",
    "                audio_lengths\n",
    "            )\n",
    "            pooled_state = mean_pool(hidden_states, feature_lengths)\n",
    "            # if attention mask does not exist, then mean-pool only all output frames\n",
    "        else:\n",
    "            pooled_state = self.pooling(hidden_states)\n",
    "\n",
    "        intermediate_state = self.intermediate_layer_dropout(pooled_state)\n",
    "        final_state = self.final_layer(intermediate_state)\n",
    "\n",
    "        return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 21:33:07.812822: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tine this model, you need a GPU or a TPU\n",
      "Loading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mloss, optimizer\u001b[39m=\u001b[39moptimizer, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> 22\u001b[0m model \u001b[39m=\u001b[39m build_model()\n",
      "Cell \u001b[0;32mIn [10], line 10\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m: tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(MAX_SEQ_LENGTH), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(MAX_SEQ_LENGTH), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[39m# Instantiate the Wav2Vec2 model w/Classification-Head using the desired\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# pre-trained checkpoint(wav2vec2-base-960h)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m wav2vec2_model \u001b[39m=\u001b[39m TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT, NUM_CLASSES)(inputs)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs, wav2vec2_model)\n",
      "Cell \u001b[0;32mIn [9], line 37\u001b[0m, in \u001b[0;36mTFWav2Vec2ForAudioClassification.__init__\u001b[0;34m(self, model_checkpoint, num_classes)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39msuper\u001b[39m(TFWav2Vec2ForAudioClassification, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     36\u001b[0m \u001b[39m# Instantiate the Wav2vec2 model w/out the Classification-head\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwav2vec2 \u001b[39m=\u001b[39m TFWav2Vec2Model\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     38\u001b[0m     model_checkpoint, apply_spec_augment\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, from_pt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mGlobalAveragePooling1D()\n\u001b[1;32m     42\u001b[0m \u001b[39m# Drop-out layer before the classification head\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorFlow/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:2682\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_pytorch_checkpoint_in_tf2_model\n\u001b[1;32m   2681\u001b[0m     \u001b[39m# Load from a PyTorch checkpoint\u001b[39;00m\n\u001b[0;32m-> 2682\u001b[0m     \u001b[39mreturn\u001b[39;00m load_pytorch_checkpoint_in_tf2_model(\n\u001b[1;32m   2683\u001b[0m         model, resolved_archive_file, allow_missing_keys\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_loading_info\u001b[39m=\u001b[39;49moutput_loading_info\n\u001b[1;32m   2684\u001b[0m     )\n\u001b[1;32m   2685\u001b[0m \u001b[39melif\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m   2686\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorFlow/lib/python3.10/site-packages/transformers/modeling_tf_pytorch_utils.py:152\u001b[0m, in \u001b[0;36mload_pytorch_checkpoint_in_tf2_model\u001b[0;34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     logger\u001b[39m.\u001b[39merror(\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLoading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    # Model's Input\n",
    "    inputs = {\n",
    "        \"input_values\": tf.keras.Input(shape=(MAX_SEQ_LENGTH), dtype=\"float32\"),\n",
    "        \"attention_mask\": tf.keras.Input(shape=(MAX_SEQ_LENGTH), dtype=\"float32\"),\n",
    "    }\n",
    "\n",
    "    # Instantiate the Wav2Vec2 model w/Classification-Head using the desired\n",
    "    # pre-trained checkpoint(wav2vec2-base-960h)\n",
    "    wav2vec2_model = TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT, NUM_CLASSES)(inputs)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Model(inputs, wav2vec2_model)\n",
    "    # Loss \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    # Optimizer\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    # Compile and Return\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove targets from the training dictionaries\n",
    "train_x = {x:y for x, y in train.items() if x!=\"label\"}\n",
    "test_x = {x: y for x, y in test.items() if x != \"label\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     train_x,\n\u001b[1;32m      3\u001b[0m     train[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     validation_data\u001b[39m=\u001b[39m(test_x, test[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      6\u001b[0m     epochs\u001b[39m=\u001b[39mMAX_EPOCHS,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_x,\n",
    "    train[\"label\"],\n",
    "    validation_data=(test_x, test[\"label\"]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('TensorFlow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c48fb264bba0529b917885aa2fdf54bfc5ac58ac8ea30a57d1df6ad7c47fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
